---
title: "Projektbeschreibung - AOT"
author: "Martin Zaefferer"
output:
  pdf_document: default
  html_document: 
    theme: readable
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: yes
    highlight: pygments
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Einführung
Diese Projektaufgabe ist Grundlage für die Prüfungsleistung der Lehreinheit Applied Optimization Techniques.

## Aufgabenstellung
- In einer Gruppenarbeit soll ein Problem aus dem Bereich des Machine Learnings 
untersucht und mit Optimierungsalgorithmen gelöst werden: Maximum Likelihood Estimation (MLE)
für Gaußsche Prozessmodelle.
- Jede Gruppe (3-4 Personen) untersucht mindestens einen spezifischen Optimierungsalgorithmus und vergleicht diesen mit der Defaultlösung (siehe weiter unten im Dokument).
- Wer möchte, kann selbst einen Algorithmus wählen (dann aber bitte absprechen,
damit nicht alle das gleiche machen). Sonst wird ein Algorithmus vorgegeben.
Eine Liste möglicher Algorithmen ist am Ende des Dokumentes zu finden.
- Mögliche Fragestellungen für die Untersuchung:
    - Wie funktioniert der Algorithmus?
    - Ist der Algorithmus für das Problem geeignet?
    - Wie erfolgt die Anwendung auf das Problem?
    - \textbf{Empirische Untersuchung:} Wie *gut* funktioniert der Algorithmus?
    - ... im Vergleich zur Defaultlösung?
    - Wie könnte die Güte verbessert werden? (z.B. Änderung der Konfiguration, andere Algorithmen,...)
    - Was passiert, wenn sich das Problem ändert (andere Daten, höhere Dimension, mehr Beobachtungen, ...)
- Jede Gruppe erstellt eine schriftliche, wissenschaftliche Ausarbeitung (Projektbericht) im Umfang von 3-6 Seiten pro Person (also bei 3 Personen z.B. insgesamt 9 - 18 Seiten).
    - Seitenzahl ohne Abbildungen / Tabellen / etc.
- Deadline für Einreichung: 15.08.2022
- Einreichung elektronisch als PDF, z.B. per Mail an [zaefferer\@dhbw-ravensburg.de](mailto:zaefferer@dhbw-ravensburg.de){.email}

# Details zum MLE Problem
Supervised Machine Learning (ML) Modelle versuchen einen Zusammenhang zwischen
Eingaben und Ausgabevariable zu erlernen. Wir betrachten hier die Regression
(Ausgabevariable ist reellwertig).

Viele ML Modelle ermöglichen eine statistische oder probabilistische Interpretation.
Das bedeutet unter anderem, dass das Modell eine Wahrscheinlichkeit (Likelihood) für beobachtete Daten abschätzen kann.
Diese Wahrscheinlichkeitsschätzung kann auch für das Training des Modells verwendet werden:
die Parameter des Modells werden so gewählt, dass die Wahrscheinlichkeit der beobachteten
Daten maximal wird. Dies wird Maximum Likelihood Estimation genannt
und stellt ein Optimierungsproblem dar, bei dem die Parameter des Modells
die zu optimierenden Variablen sind. 
Die Likelihood stellt den Zielfunktionswert dar.

Wir betrachten hier ein Gaußsches Prozessmodell (GPM) (auch: Gaussian process regression oder Kriging).

# Demonstration

Als Grundlage brauchen wir erst ein paar Datenpunkte, die wir hier mit
einer einfachen Testfunktion erzeugen:
```{r}
set.seed(1)
n <- 70
lower <- c(-2.5,-1.5)
upper <- c(1.5,2.5)
x <- cbind(runif(n,lower[1],upper[1]),runif(n,lower[2],upper[2]))
f <- function(x){
  20 + x[,1]^2 + x[,2]^2 - 10*(cos(2*pi*x[,1]) + cos(2*pi*x[,2]))
}
y <- f(x)
```

Hier soll modelliert werden, wie y von x.1 und x.2 abhängt (Spalten `x`).
Wir können uns die erzeugten Daten anschauen mit:

```{r}
df <- data.frame(x=x,y=y)
require(ggplot2)
require(viridis)
ggplot(data=df,aes(x=x.1,y=x.2,colour=y)) +
  geom_point() +
  scale_colour_viridis(option="A")
```

Mit dem Paket SPOT können wir ein GPM erzeugen ...

```{r}
require(SPOT)
model <- buildKriging(x,matrix(y,,1))
```

... und auch visualisieren ...
```{r}
nplot_dim <- 100
xplot <- expand.grid(seq(from=lower[1],to=upper[1],length.out=nplot_dim),
               seq(from=lower[2],to=upper[2],length.out=nplot_dim))
yplot <- predict(model,xplot)
df <- data.frame(x.1=xplot$Var1,x.2=xplot$Var2,y=yplot)
ggplot(data=df,aes(x=x.1,y=x.2,z=y)) +
  geom_contour_filled(bins=50,show.legend=FALSE) +
  scale_fill_viridis(option="A",discrete = T)
```

Ups, das sieht eigenartig aus.
Um zu prüfen, ob das Ergebnis 'gut' ist, können wir uns erstmal anschauen, wie die Testfunktion tatsächlich aussehen soll.

```{r}
yplot2 <- f(xplot)
df <- data.frame(x.1=xplot$Var1,x.2=xplot$Var2,y=yplot2)
ggplot(data=df,aes(x=x.1,y=x.2,z=y)) +
  geom_contour_filled(bins=50,show.legend=FALSE) +
  scale_fill_viridis(option="A",discrete = T)
```
Wie zu sehen ist, gibt es eine deutliche Abweichung zwischen Modell und Testfunktion.
Wir können uns zusätzlich anschauen, wie gut MLE funktioniert hat.
MLE wurde im Hintergrund ausgeführt, während des `buildKriging` 
Aufrufs.

Der beste gefundene Likelihood Wert ist:
```{r}
model$like
```
Anmerkung: das ist die konzentrierte, negierte, und log-transformierte Likelihood.
Kleinere Werte sind besser.


Der Wert sollte schlechter (größer) werden, wenn wir die Zahl der Zielfunktionsauswertungen reduzieren.

```{r}
model <- buildKriging(x,matrix(y,,1),control=list(
  budgetAlgTheta=1
))
model$like
model$Theta
model$Lambda
```


```{r}
yplot <- predict(model,xplot)
df <- data.frame(x.1=xplot$Var1,x.2=xplot$Var2,y=yplot)
ggplot(data=df,aes(x=x.1,y=x.2,z=y)) +
  geom_contour_filled(bins=50,show.legend=FALSE) +
  scale_fill_viridis(option="A",discrete = T)
```

CMA-ES

```{r}
cma_es <- function (x = NULL, fun, lower, upper, 
                                 control = list(), ...) {

    result <- cmaes::cma_es(par = runif(length(lower),lower,upper),
                          fn=fun,lower = lower, upper=upper, ...)
    
    list(
    xbest = result$par, 
    ybest = result$value, 
    count = 0
  )
}
```

Evaluation

```{r}
set.seed(1)
n_eval <- 50
x_eval <- cbind(runif(n_eval,lower[1],upper[1]),runif(n_eval,lower[2],upper[2]))

y_eval <- f(x_eval)
```



```{r}

likes_cma_es <- NULL
likes_default <- NULL

theta_cma_es <- NULL
theta_default <-NULL

lambda_cma_es <- NULL
lambda_default <- NULL

rmse_cma_es <- NULL
rmse_default <- NULL


for(i in 0:50){
  model_cmaes <- buildKriging(x,matrix(y,,1),control=list(
      algTheta=cma_es
  ))
    
  model_default <- buildKriging(x,matrix(y,,1))
  
  likes_cma_es <- rbind(likes_cma_es, model_cmaes$like)
  likes_default <- rbind(likes_default, model_default$like)
  
  
  theta_cma_es <- rbind(theta_cma_es, model_cmaes$Theta)
  theta_default <- rbind(theta_default, model_default$Theta)
  
  lambda_cma_es <- rbind(lambda_cma_es, model_cmaes$Lambda)
  lambda_default <- rbind(lambda_default, model_default$Lambda)
  
  y_cmaes <- predict(model_cmaes, x_eval)
  y_default <- predict(model_default, x_eval)
  
  y_ev <- cbind(y_eval)
  y_cm <- NULL
  y_def <- NULL
  
  for(element in y_cmaes){
    y_cm <- cbind(y_cm, element)
  }
  
  for(element in y_default){
    y_def <- cbind(y_def, element)
  }
  
  rmse_cma_es <- cbind(rmse_cma_es, sqrt(mean((y_ev - y_cm)^2)))
  rmse_default <- cbind(rmse_default, sqrt(mean((y_ev - y_def)^2)))

}

likes_cma_es
likes_default

rmse_cma_es
rmse_default

```


```{r}

likes_cma_es[]

```



```{r}

#theta_cma_es 
#theta_default

theta_like_cma_es <- cbind(theta_cma_es, likes_cma_es)
theta_like_cma_es

#lambda_cma_es
#lambda_default


```



```{r}
df <- data.frame(x=theta_cma_es,y=likes_cma_es)
require(ggplot2)
require(viridis)
ggplot(data=df,aes(x=x.1,y=x.2,colour=y)) +
  geom_point() +
  scale_colour_viridis(option="A")
```
```{r}
df <- data.frame(x=theta_default,y=likes_default)
require(ggplot2)
require(viridis)
ggplot(data=df,aes(x=x.1,y=x.2,colour=y)) +
  geom_point() +
  scale_colour_viridis(option="A")
```



```{r}

boxplot(likes_cma_es[,1], likes_default[,1],horizontal=T,names=c("CMA-ES","Default"))
boxplot(rmse_cma_es[1,], rmse_default[1,],horizontal=T,names=c("CMA-ES","Default"))

```



```{r}

likes_cma_es <- NULL
likes_default <- NULL

rmse_cma_es <- NULL
rmse_default <- NULL


for(j in 0:20){
  set.seed(1)
  n <- 70
  x <- cbind(runif(n,lower[1],upper[1]),runif(n,lower[2],upper[2]))
  y <- f(x)
  
  
  set.seed(1)
  n_eval <- 50
  x_eval <- cbind(runif(n_eval,lower[1],upper[1]),runif(n_eval,lower[2],upper[2]))
  y_eval <- f(x_eval)
  
  
    for(i in 0:50){
    model_cmaes <- buildKriging(x,matrix(y,,1),control=list(
        algTheta=cma_es
    ))
      
    model_default <- buildKriging(x,matrix(y,,1))
    
    likes_cma_es <- rbind(likes_cma_es, model_cmaes$like)
    likes_default <- rbind(likes_default, model_default$like)
    
    
    y_cmaes <- predict(model_cmaes, x_eval)
    y_default <- predict(model_default, x_eval)
    
    y_ev <- cbind(y_eval)
    y_cm <- NULL
    y_def <- NULL
    
    for(element in y_cmaes){
      y_cm <- cbind(y_cm, element)
    }
    
    for(element in y_default){
      y_def <- cbind(y_def, element)
    }
    
    rmse_cma_es <- cbind(rmse_cma_es, sqrt(mean((y_ev - y_cm)^2)))
    rmse_default <- cbind(rmse_default, sqrt(mean((y_ev - y_def)^2)))
  
  }
  
  
}



#likes_cma_es
#likes_default

#rmse_cma_es
#rmse_default

```

```{r}

boxplot(rmse_cma_es[1,], rmse_default[1,],horizontal=T,names=c("CMA-ES","Default"))

```
